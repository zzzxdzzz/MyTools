{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe3e815-9d1d-470d-936d-d22e98864490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AkShare version: 1.17.61\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import timedelta\n",
    "import akshare as ak\n",
    "import os, time, math, random\n",
    "import requests\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"AkShare version:\", ak.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82bf08b3-a4da-4ed0-bd07-37afa07bf173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- å¸¸ç”¨å°å·¥å…· ----------\n",
    "\n",
    "def normalize_ymd(s: str) -> str:\n",
    "    \"\"\"\n",
    "    å°†å½¢å¦‚ YYYYMMDD çš„å­—ç¬¦ä¸²è§„æ•´ä¸º**å­˜åœ¨çš„æ—¥æœŸ**ã€‚\n",
    "    è‹¥æ—¥>å½“æœˆå¤©æ•°ï¼Œåˆ™å–å½“æœˆæœ€åä¸€å¤©ï¼›è‹¥æ ¼å¼å¼‚å¸¸åˆ™æŠ›é”™ã€‚\n",
    "    \"\"\"\n",
    "    s = re.sub(r\"\\D\", \"\", s)\n",
    "    if len(s) != 8:\n",
    "        raise ValueError(f\"éæ³•æ—¥æœŸï¼š{s}\")\n",
    "    y, m, d = int(s[:4]), int(s[4:6]), int(s[6:])\n",
    "    if not (1 <= m <= 12):\n",
    "        raise ValueError(f\"éæ³•æœˆä»½ï¼š{s}\")\n",
    "    dim = monthrange(y, m)[1]\n",
    "    d = min(max(d, 1), dim)\n",
    "    return f\"{y:04d}{m:02d}{d:02d}\"\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    if not os.path.exists(p):\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def std_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"æŠŠ AkShare çš„ä¸­è‹±åˆ—åç»Ÿä¸€æˆè‹±æ–‡ï¼Œå¹¶è®¾ç½® datetime ç´¢å¼•ã€‚\"\"\"\n",
    "    colmap = {\n",
    "        \"æ—¥æœŸ\":\"date\",\"date\":\"date\",\n",
    "        \"å¼€ç›˜\":\"open\",\"open\":\"open\",\n",
    "        \"æœ€é«˜\":\"high\",\"high\":\"high\",\n",
    "        \"æœ€ä½\":\"low\",\"low\":\"low\",\n",
    "        \"æ”¶ç›˜\":\"close\",\"close\":\"close\",\n",
    "        \"æˆäº¤é‡\":\"volume\",\"volume\":\"volume\",\n",
    "        \"æˆäº¤é¢\":\"amount\",\"amount\":\"amount\",\n",
    "    }\n",
    "    df = df.rename(columns={c: colmap.get(c, c) for c in df.columns})\n",
    "    if \"date\" not in df.columns or \"close\" not in df.columns:\n",
    "        raise RuntimeError(f\"ç¼ºå°‘å¿…è¦åˆ—ï¼š{list(df.columns)}\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    for k in [\"open\",\"high\",\"low\",\"close\",\"volume\",\"amount\"]:\n",
    "        if k in df.columns:\n",
    "            df[k] = pd.to_numeric(df[k], errors=\"coerce\")\n",
    "    df = df.sort_values(\"date\").drop_duplicates(subset=[\"date\"]).set_index(\"date\")\n",
    "    return df\n",
    "\n",
    "def load_a_share_codes() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    è¯»å– A è‚¡ä»£ç è¡¨ï¼Œè¿”å› DataFrame(['code','name'])ã€‚\n",
    "    è¿‡æ»¤å‡º 6 ä½æ•°å­—ä»£ç ï¼ˆä¸»æ¿/åˆ›ä¸š/ç§‘åˆ›ï¼‰ï¼Œä¸å«åŒ—äº¤æ‰€çš„ 8 å¼€å¤´ç­‰ã€‚\n",
    "    \"\"\"\n",
    "    tab = ak.stock_info_a_code_name()  # å« A è‚¡ä¸»æ¿/åˆ›ä¸š/ç§‘åˆ›\n",
    "    if {\"code\",\"name\"}.issubset(tab.columns):\n",
    "        df = tab[[\"code\",\"name\"]].copy()\n",
    "    elif {\"è¯åˆ¸ä»£ç \",\"è¯åˆ¸ç®€ç§°\"}.issubset(tab.columns):\n",
    "        df = tab.rename(columns={\"è¯åˆ¸ä»£ç \":\"code\",\"è¯åˆ¸ç®€ç§°\":\"name\"})[[\"code\",\"name\"]].copy()\n",
    "    else:\n",
    "        raise RuntimeError(f\"æ— æ³•è¯†åˆ«ä»£ç /åç§°åˆ—ï¼š{list(tab.columns)}\")\n",
    "    df[\"code\"] = df[\"code\"].astype(str).str.strip()\n",
    "    df = df[df[\"code\"].str.fullmatch(r\"\\d{6}\")]\n",
    "    df = df.drop_duplicates(subset=[\"code\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def fetch_hist(symbol: str, start_ymd: str, end_ymd: str, adjust: str = \"\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    å…ˆèµ° AkShareï¼›é‡åˆ°ç½‘ç»œä¸­æ–­/é™æµç±»é”™è¯¯ï¼ˆå¦‚ RemoteDisconnected/Connection/Timeoutï¼‰ï¼Œ\n",
    "    è‡ªåŠ¨å…œåº•èµ° push2his ç›´è¿ï¼›å…¶ä»–é”™è¯¯åŸæ ·æŠ›å‡ºã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        raw = ak.stock_zh_a_hist(symbol=symbol, period=\"daily\",\n",
    "                                 start_date=start_ymd, end_date=end_ymd, adjust=adjust)\n",
    "        return std_cols(raw)\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        transient = (\"Remote end closed\" in msg) or (\"Connection\" in msg) or (\"Timeout\" in msg)\n",
    "        if transient:\n",
    "            # å…œåº•ç›´è¿ï¼ˆä¾æ—§å¤±è´¥ä¼šæŠ› RuntimeErrorï¼‰\n",
    "            return fetch_hist_em_direct(symbol, start_ymd, end_ymd, adjust)\n",
    "        # éç½‘ç»œç±»é”™è¯¯ï¼Œç›´æ¥æŠ›å‡ºè®©ä¸Šå±‚å¤„ç†ï¼ˆæ¯”å¦‚ä»£ç æ— æ•ˆç­‰ï¼‰\n",
    "        raise\n",
    "\n",
    "def csv_path(symbol: str) -> str:\n",
    "    return os.path.join(DATA_DIR, f\"{symbol}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a18feb9-492b-4357-84a0-e504ca04dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- å¢é‡æ›´æ–°å·²æœ‰ CSV åˆ°â€œä»Šå¤©â€ ----------\n",
    "\n",
    "def update_all_to_latest(adjust: str = ADJUST, end_date: str | None = None):\n",
    "    \"\"\"\n",
    "    éå† DATA_DIR ä¸‹çš„ CSVï¼š\n",
    "    - è¯»å–æœ€åæ—¥æœŸ last_dt\n",
    "    - ä» last_dt+1 åˆ° end_dateï¼ˆé»˜è®¤ä»Šå¤©ï¼‰æ‹‰å–å¢é‡å¹¶åˆå¹¶å»é‡\n",
    "    \"\"\"\n",
    "    ensure_dir(DATA_DIR)\n",
    "    today = datetime.now().strftime(\"%Y%m%d\") if end_date is None else normalize_ymd(end_date)\n",
    "    files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".csv\")]\n",
    "    print(f\"å¾…æ›´æ–°æ–‡ä»¶æ•°ï¼š{len(files)}ï¼Œç›®æ ‡æˆªæ­¢ï¼š{today}\")\n",
    "\n",
    "    for idx, fn in enumerate(sorted(files), 1):\n",
    "        sym = fn.replace(\".csv\",\"\")\n",
    "        path = os.path.join(DATA_DIR, fn)\n",
    "        try:\n",
    "            df_old = pd.read_csv(path, parse_dates=[\"date\"], dtype={\"open\":float,\"high\":float,\"low\":float,\"close\":float,\"volume\":float,\"amount\":float})\n",
    "            df_old = df_old.set_index(\"date\").sort_index()\n",
    "            if df_old.empty:\n",
    "                start = normalize_ymd(START_DATE_RAW)\n",
    "            else:\n",
    "                last_dt = df_old.index.max()\n",
    "                start = (last_dt + timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "\n",
    "            if start > today:\n",
    "                print(f\"[{idx}/{len(files)}] è·³è¿‡ {sym}ï¼ˆå·²æœ€æ–°ï¼‰\")\n",
    "                continue\n",
    "\n",
    "            df_new = fetch_hist(sym, start, today, adjust)\n",
    "            if df_new.empty:\n",
    "                print(f\"[{idx}/{len(files)}] {sym} æ— å¢é‡\")\n",
    "                continue\n",
    "\n",
    "            merged = pd.concat([df_old, df_new]).sort_index()\n",
    "            merged = merged[~merged.index.duplicated(keep=\"last\")]\n",
    "            merged.to_csv(path, index=True)\n",
    "            print(f\"[{idx}/{len(files)}] æ›´æ–° {sym}ï¼š+{len(df_new)} æ¡ï¼Œåˆè®¡ {len(merged)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx}/{len(files)}] æ›´æ–°å¤±è´¥ {sym}ï¼š{repr(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1e3ea7d-46e6-486b-a10c-6267ad82c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- æ¨¡å—ä¸‰ï¼šç”»æŒ‡å®šè‚¡ç¥¨â€œè¿‡å»ä¸¤ä¸ªæœˆâ€çš„ä¸‰æ¡å‡çº¿ ----------\n",
    "\n",
    "def plot_three_ma(symbol: str, win_d=WIN_DAILY, win_w=WIN_WEEKLY, win_m=WIN_MONTH):\n",
    "    \"\"\"\n",
    "    ä» CSVï¼ˆè‹¥ä¸å­˜åœ¨åˆ™åœ¨çº¿æŠ“å–ï¼‰è¯»å–æ•°æ®ï¼›\n",
    "    å–â€œæœ€è¿‘ä¸¤ä¸ªæœˆï¼ˆæŒ‰æ—¥å†æœˆï¼‰â€åŒºé—´ï¼Œç”» 3 æ¡å‡çº¿ï¼ˆä¸ç”»Kçº¿ï¼‰ã€‚\n",
    "    \"\"\"\n",
    "    # å­—ä½“ï¼šå°½é‡ç”¨ç³»ç»Ÿä¸­æ–‡ï¼Œé¿å…å£å­—æ¡†ï¼ˆå¤±è´¥ä¹Ÿä¸æŠ¥é”™ï¼‰\n",
    "    try:\n",
    "        from matplotlib import font_manager, rcParams\n",
    "        rcParams[\"axes.unicode_minus\"] = False\n",
    "        for name in [\"PingFang SC\",\"Hiragino Sans GB\",\"Microsoft YaHei\",\"SimHei\",\"Noto Sans CJK SC\",\"Arial Unicode MS\"]:\n",
    "            try:\n",
    "                font_manager.findfont(font_manager.FontProperties(family=name), fallback_to_default=False)\n",
    "                rcParams[\"font.sans-serif\"] = [name]; break\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    path = csv_path(symbol)\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path, parse_dates=[\"date\"]).set_index(\"date\").sort_index()\n",
    "    else:\n",
    "        df = fetch_hist(symbol, normalize_ymd(START_DATE_RAW), normalize_ymd(END_DATE_RAW), ADJUST)\n",
    "\n",
    "    # è®¡ç®—ä¸‰æ¡å‡çº¿ï¼ˆç®€å•ç§»åŠ¨å‡çº¿ï¼‰\n",
    "    px = df[\"close\"].astype(float)\n",
    "    ma_d = px.rolling(win_d, min_periods=1).mean()\n",
    "    ma_w = px.rolling(win_w, min_periods=1).mean()\n",
    "    ma_m = px.rolling(win_m, min_periods=1).mean()\n",
    "\n",
    "    # è¿‡å»ä¸¤ä¸ªæœˆï¼ˆæŒ‰æ—¥å†ï¼‰\n",
    "    last_day = px.index.max()\n",
    "    start_dt = (last_day - pd.DateOffset(months=2)).normalize()\n",
    "    sl = slice(start_dt, last_day)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(ma_d.loc[sl].index, ma_d.loc[sl].values, label=f\"{win_d}æ—¥å‡çº¿\")\n",
    "    plt.plot(ma_w.loc[sl].index, ma_w.loc[sl].values, label=f\"{win_w}æ—¥å‡çº¿(â‰ˆå‘¨)\")\n",
    "    plt.plot(ma_m.loc[sl].index, ma_m.loc[sl].values, label=f\"{win_m}æ—¥å‡çº¿(â‰ˆæœˆ)\")\n",
    "    plt.title(f\"{symbol} è¿‡å»ä¸¤ä¸ªæœˆä¸‰æ¡å‡çº¿\")\n",
    "    plt.xlabel(\"æ—¥æœŸ\"); plt.ylabel(\"ä»·æ ¼\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15590330-3b08-45ed-a790-657d48741d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_bull_cross_symbols180(\n",
    "    win_s1: int = 5,\n",
    "    win_s2: int = 10,\n",
    "    win_s3: int = 20,\n",
    "    win_l: int = 60,\n",
    "    lookback: int = 1,\n",
    "    data_dir: str = None,   # è‹¥ä¸º None åˆ™ç”¨å…¨å±€ DATA_DIR\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    åœ¨ data_dir ä¸‹æ‰€æœ‰ CSV ä¸­ï¼ŒæŸ¥æ‰¾æ»¡è¶³ï¼š\n",
    "      - æœ€è¿‘ 'lookback' æ ¹å†…ï¼ŒMA5ã€MA10ã€MA20 å„è‡ªéƒ½å‡ºç°â€œä»ä¸‹å‘ä¸Šçªç ´ MA60â€çš„é‡‘å‰\n",
    "      - ä¸”å½“å‰ï¼ˆæœ€åä¸€æ ¹ï¼‰ MA5 > MA60ï¼ŒMA10 > MA60ï¼ŒMA20 > MA60\n",
    "    è¿”å›ä»£ç åˆ—è¡¨ã€‚\n",
    "\n",
    "    å½“æ ·æœ¬é•¿åº¦ < 60 æ—¶ï¼Œä½¿ç”¨â€œçº¿æ€§è¶‹åŠ¿åæ¨ + ç­‰æƒè¡¥å…¨â€çš„æ–¹å¼ä¼°ç®—ä¸¥æ ¼å£å¾„çš„ MA60ï¼š\n",
    "      1) ç”¨ä¸€é˜¶çº¿æ€§å›å½’æ‹Ÿåˆæ”¶ç›˜ä»·éšæ—¶é—´çš„è¶‹åŠ¿ï¼›\n",
    "      2) å‘è¿‡å»å¤–æ¨ (60 - n) ä¸ªç‚¹ä½œä¸ºâ€œè™šæ‹Ÿå†å²â€ï¼Œå¹¶è¿›è¡Œæ¸©å’Œæˆªæ–­ä»¥é¿å…æç«¯å€¼ï¼›\n",
    "      3) å°†â€œè™šæ‹Ÿå†å² + å®é™…æ•°æ®â€æ‹¼æ¥åï¼Œåš 60 é•¿åº¦ç­‰æƒæ»‘åŠ¨å¹³å‡ï¼Œå†ä¸åŸå§‹ç´¢å¼•å¯¹é½ã€‚\n",
    "    \"\"\"\n",
    "    if data_dir is None:\n",
    "        data_dir = DATA_DIR  # å…¼å®¹ä½ ç°æœ‰çš„å…¨å±€å˜é‡\n",
    "\n",
    "    def estimate_ma60_strict(px: pd.Series, win: int) -> pd.Series:\n",
    "        \"\"\"\n",
    "        è‹¥ len(px) >= win: ç›´æ¥æ»šåŠ¨å‡çº¿ï¼ˆä¸¥æ ¼ç­‰æƒï¼‰ï¼›\n",
    "        è‹¥ len(px) <  win: ç”¨çº¿æ€§è¶‹åŠ¿å‘è¿‡å»å¤–æ¨ (win-len) ä¸ªç‚¹è¡¥å…¨åï¼Œå†åšä¸¥æ ¼ç­‰æƒçš„ rolling(win)ã€‚\n",
    "        è¿”å›ä¸ px åŒä¸€ç´¢å¼•å¯¹é½çš„ç­‰æƒ MA(win)ã€‚\n",
    "        \"\"\"\n",
    "        n = len(px)\n",
    "        if n == 0:\n",
    "            return pd.Series(index=px.index, dtype=float)\n",
    "\n",
    "        if n >= win:\n",
    "            return px.rolling(win, min_periods=win).mean().reindex(px.index)\n",
    "\n",
    "        # â€”â€” n < win: æ„é€ â€œè™šæ‹Ÿå†å²â€ â€”â€” #\n",
    "        # ç”¨æ•´æ•°æ—¶é—´ç´¢å¼•æ‹Ÿåˆçº¿æ€§è¶‹åŠ¿ y = a + b*t\n",
    "        t_obs = np.arange(n, dtype=float)\n",
    "        y_obs = px.values.astype(float)\n",
    "\n",
    "        # å¤„ç†å…¨å¸¸æ•°æˆ–æ–¹å·®æå°æƒ…å†µï¼Œé¿å…å¥‡å¼‚è§£\n",
    "        if np.allclose(y_obs, y_obs[0], rtol=0, atol=1e-12):\n",
    "            a, b = float(y_obs[0]), 0.0\n",
    "        else:\n",
    "            a, b = np.polyfit(t_obs, y_obs, 1)\n",
    "\n",
    "        need = win - n\n",
    "        t_back = np.arange(-need, 0, dtype=float)  # è´Ÿçš„æ—¶é—´æ­¥ï¼Œå‘è¿‡å»å¤–æ¨\n",
    "        y_back = a + b * t_back\n",
    "\n",
    "        # æ¸©å’Œ winsorizeï¼šä»¥é¦–å°¾è§‚æµ‹å€¼çš„èŒƒå›´åš 20% ä½™é‡åŒ…ç»œï¼Œé¿å…å¤–æ¨å¤±çœŸè¿‡å¤§\n",
    "        lo = float(min(y_obs.min(), y_obs.max()) * 0.8)\n",
    "        hi = float(max(y_obs.min(), y_obs.max()) * 1.2)\n",
    "        y_back = np.clip(y_back, lo, hi)\n",
    "\n",
    "        # æ‹¼æ¥ï¼šè™šæ‹Ÿå†å² + å®é™…æ•°æ®ï¼ˆç´¢å¼•ç”¨ç®€å•æ•´æ•°ï¼Œç¨åå¯¹é½å›åŸç´¢å¼•ï¼‰\n",
    "        y_ext = np.concatenate([y_back, y_obs])  # é•¿åº¦æ°å¥½ = win\n",
    "        idx_ext = np.arange(-need, n, dtype=int)\n",
    "        s_ext = pd.Series(y_ext, index=idx_ext, dtype=float)\n",
    "\n",
    "        # åœ¨æ‰©å±•åºåˆ—ä¸Šåšä¸¥æ ¼ç­‰æƒ rolling(win)\n",
    "        ma_ext = s_ext.rolling(win, min_periods=win).mean()\n",
    "\n",
    "        # å°†æ‰©å±•çš„ rolling å‡çº¿å¯¹é½åˆ°åŸå§‹è§‚æµ‹ç´¢å¼•ï¼ˆ0..n-1ï¼‰\n",
    "        ma_on_obs = ma_ext.reindex(np.arange(0, n))\n",
    "        ma_on_obs.index = px.index  # å¯¹é½å›æ—¥æœŸç´¢å¼•\n",
    "        return ma_on_obs\n",
    "\n",
    "    def crossed_up(short_ma: pd.Series, long_ma: pd.Series, lb: int) -> bool:\n",
    "        # åœ¨è¿‡å» lb æ ¹é‡Œï¼Œå­˜åœ¨æŸä¸€æ ¹ kï¼š short<=longï¼Œä¸‹ä¸€æ ¹ short>long\n",
    "        idx_len = len(long_ma)\n",
    "        if idx_len < 2:\n",
    "            return False\n",
    "        for k in range(1, lb + 1):\n",
    "            i_prev = -1 - k\n",
    "            i_curr = -k\n",
    "            if abs(i_prev) > idx_len or abs(i_curr) > idx_len:\n",
    "                break\n",
    "            sp, lp = float(short_ma.iloc[i_prev]), float(long_ma.iloc[i_prev])\n",
    "            sc, lc = float(short_ma.iloc[i_curr]), float(long_ma.iloc[i_curr])\n",
    "            if (sp <= lp) and (sc > lc):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    out = []\n",
    "    files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "    for fn in files:\n",
    "        sym = fn[:-4]\n",
    "        try:\n",
    "            df = (\n",
    "                pd.read_csv(os.path.join(data_dir, fn), parse_dates=[\"date\"])\n",
    "                .set_index(\"date\")\n",
    "                .sort_index()\n",
    "            )\n",
    "            if \"close\" not in df.columns:\n",
    "                continue\n",
    "\n",
    "            px = df[\"close\"].astype(float)\n",
    "            n = len(px)\n",
    "            if n < 2:\n",
    "                continue\n",
    "\n",
    "            # è®¡ç®—çŸ­å‡çº¿ï¼ˆå…è®¸çŸ­æ ·æœ¬ï¼Œmin_periods=1 ä¿è¯æœ‰å€¼ç”¨äºâ€œå½“å‰>MA60â€çš„åˆ¤å®šï¼‰\n",
    "            ma5  = px.rolling(win_s1, min_periods=1).mean()\n",
    "            ma10 = px.rolling(win_s2, min_periods=1).mean()\n",
    "            ma20 = px.rolling(win_s3, min_periods=1).mean()\n",
    "\n",
    "            # è®¡ç®—â€œä¸¥æ ¼å£å¾„â€çš„ MA60ï¼ˆä¸è¶³ 60 ç”¨ä¼°ç®—æ³•è¡¥å…¨åå† rollingï¼‰\n",
    "            ma60 = estimate_ma60_strict(px, win_l)\n",
    "\n",
    "            # è‹¥ä¼°ç®—è¿”å›æœ‰ç¼ºå¤±ï¼ˆæç«¯è¾¹ç•Œï¼‰ï¼Œè·³è¿‡\n",
    "            if ma60.isna().all():\n",
    "                continue\n",
    "\n",
    "            curr_idx = px.index[-1]\n",
    "\n",
    "            # å½“å‰å¿…é¡»éƒ½åœ¨ MA60 ä¸Šæ–¹\n",
    "            if not (\n",
    "                (ma5.loc[curr_idx]  > ma60.loc[curr_idx]) and\n",
    "                (ma10.loc[curr_idx] > ma60.loc[curr_idx]) and\n",
    "                (ma20.loc[curr_idx] > ma60.loc[curr_idx])\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            # æœ€è¿‘ lookback æ ¹å‡ºç°å„è‡ªçš„â€œä»ä¸‹å‘ä¸Šâ€é‡‘å‰\n",
    "            cond5  = crossed_up(ma5,  ma60, lookback)\n",
    "            cond10 = crossed_up(ma10, ma60, lookback)\n",
    "            cond20 = crossed_up(ma20, ma60, lookback)\n",
    "\n",
    "            if cond5 and cond10 and cond20:\n",
    "                out.append(sym)\n",
    "\n",
    "        except Exception:\n",
    "            # å•åªå¼‚å¸¸ä¸å½±å“æ•´ä½“æ‰«æ\n",
    "            continue\n",
    "\n",
    "    return sorted(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d409338f-859a-459a-8ea3-1659a528421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= ç¤ºä¾‹è°ƒç”¨ =================\n",
    "# 1) æ‰¹é‡ä¸‹è½½ï¼ˆåªéœ€è·‘ä¸€æ¬¡ï¼‰\n",
    "# batch_download_all(START_DATE_RAW, END_DATE_RAW, ADJUST)\n",
    "\n",
    "# 2) å¢é‡æ›´æ–°åˆ°ä»Šå¤©ï¼ˆå¯ä»¥éš”å¤©å°±è·‘ä¸€ä¸‹ï¼‰\n",
    "# update_all_to_latest(ADJUST)\n",
    "\n",
    "# 3) ç”»å•åªè‚¡ç¥¨æœ€è¿‘ä¸¤ä¸ªæœˆçš„ 3 æ¡å‡çº¿\n",
    "# plot_three_ma(\"600519\")  # æ”¹æˆä½ æƒ³çœ‹çš„ä»£ç \n",
    "\n",
    "# 4) æ‰¾å‡ºâ€œæœˆå‡çº¿è¢«å‘¨çº¿å’Œæ—¥å‡çº¿ä»ä¸‹å‘ä¸Šçªç ´â€çš„è‚¡ç¥¨å¹¶æ‰“å°\n",
    "# symbols = find_bull_cross_symbols(lookback=1)\n",
    "# print(\"è§¦å‘ä¿¡å·çš„è‚¡ç¥¨ï¼š\", symbols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d708f6f6-8663-43a7-bbf3-2856ead71781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD---------- æ¨¡å—å››ï¼šç­›é€‰â€œæœˆå‡çº¿è¢«å‘¨çº¿å’Œæ—¥å‡çº¿ä»ä¸‹å‘ä¸Šçªç ´â€çš„è‚¡ç¥¨ ----------\n",
    "\n",
    "def find_bull_cross_symbols(win_d=WIN_DAILY, win_w=WIN_WEEKLY, win_m=WIN_MONTH, lookback=1) -> list[str]:\n",
    "    \"\"\"\n",
    "    åœ¨ DATA_DIR ä¸‹æ‰€æœ‰ CSV ä¸­ï¼ŒæŸ¥æ‰¾æ»¡è¶³ï¼š\n",
    "      - æœ€è¿‘ 'lookback' æ ¹Kçº¿å†…ï¼Œ5æ—¥ä¸10æ—¥å‡çº¿å‡ä»ä¸‹ç©¿è¶Šåˆ°ä¸Šæ–¹ 20 æ—¥å‡çº¿ï¼ˆå„è‡ªâ€œé‡‘å‰â€ï¼‰\n",
    "      - ä¸”å½“å‰ï¼ˆæœ€åä¸€æ ¹ï¼‰ ma_d > ma_m ä¸” ma_w > ma_m\n",
    "    è¿”å›ä»£ç åˆ—è¡¨ã€‚\n",
    "    è¯´æ˜ï¼šé»˜è®¤ lookback=1 è¡¨ç¤ºâ€œåˆšåˆšå‘ç”Ÿåœ¨ä¸Šä¸€æ ¹â†’å½“å‰æ ¹ä¸Šâ€ï¼ˆæœ€ä¸¥æ ¼ï¼‰ã€‚\n",
    "         è‹¥è¦æ”¾å®½ä¸ºæœ€è¿‘ 3 æ ¹å†…å‘ç”Ÿï¼ŒæŠŠ lookback=3ã€‚\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".csv\")]\n",
    "    for fn in files:\n",
    "        sym = fn.replace(\".csv\",\"\")\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(DATA_DIR, fn), parse_dates=[\"date\"]).set_index(\"date\").sort_index()\n",
    "            if len(df) < max(win_d, win_w, win_m) + 2:\n",
    "                continue\n",
    "            px = df[\"close\"].astype(float)\n",
    "            ma_d = px.rolling(win_d, min_periods=1).mean()\n",
    "            ma_w = px.rolling(win_w, min_periods=1).mean()\n",
    "            ma_m = px.rolling(win_m, min_periods=1).mean()\n",
    "\n",
    "            # æœ€è¿‘ä¸¤æ ¹ï¼šprev, curr\n",
    "            curr = ma_d.index[-1]\n",
    "            prev = ma_d.index[-2]\n",
    "\n",
    "            # å½“å‰å¿…é¡»åœ¨ä¸Šæ–¹\n",
    "            if not (ma_d.loc[curr] > ma_m.loc[curr] and ma_w.loc[curr] > ma_m.loc[curr]):\n",
    "                continue\n",
    "\n",
    "            # â€œä»ä¸‹å‘ä¸Šçªç ´â€æ£€æµ‹ï¼ˆæœ€è¿‘ lookback æ ¹å†…ï¼‰\n",
    "            def crossed_up(short_ma, long_ma, lb=lookback):\n",
    "                # åœ¨è¿‡å» lb æ ¹é‡Œï¼Œå­˜åœ¨æŸä¸€æ ¹ kï¼š short<=longï¼Œä¸‹ä¸€æ ¹ short>long\n",
    "                idx = long_ma.index\n",
    "                for k in range(1, lb+1):\n",
    "                    i_prev = -1 - k\n",
    "                    i_curr = -k\n",
    "                    if i_prev < -len(idx):  # é•¿åº¦ä¸å¤Ÿ\n",
    "                        break\n",
    "                    if (short_ma.iloc[i_prev] <= long_ma.iloc[i_prev]) and (short_ma.iloc[i_curr] > long_ma.iloc[i_curr]):\n",
    "                        return True\n",
    "                return False\n",
    "\n",
    "            if crossed_up(ma_d, ma_m, lookback) and crossed_up(ma_w, ma_m, lookback):\n",
    "                out.append(sym)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "    return sorted(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "009de9a3-c2d9-4b95-aa35-fbdbdbef6566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_download_all_minimal(\n",
    "    start_raw: str, end_raw: str, adjust: str = ADJUST,\n",
    "    start_at: int = 1, resume_from_symbol: str | None = None,\n",
    "    # â€”â€” é€Ÿç‡ä¸å†·å´ â€”â€” \n",
    "    per_item_sleep: tuple[float,float] = (0.6, 1.2),\n",
    "    rate_per_min: int = 15,\n",
    "    cooldown_every: int = 60,\n",
    "    cooldown_seconds: tuple[float,float] | float = (900, 1800),  # æ›´æ¸©å’Œçš„å¼ºåˆ¶å†·å´\n",
    "    # â€”â€” ä¼šè¯ä¸æ–­è·¯å™¨ â€”â€” \n",
    "    session_refresh_every: int = 120,\n",
    "    circuit_fail_window: int = 5,\n",
    "    circuit_fail_threshold: int = 3,\n",
    "    circuit_sleep_range: tuple[float,float] = (1200, 1800),\n",
    "    # â€”â€” é‡è¯• â€”â€” \n",
    "    retry_per_symbol: int = 3,\n",
    "    # â€”â€” ç¼ºå£åˆ¤å®šï¼ˆå†…éƒ¨åˆ†æ®µï¼‰â€”â€”\n",
    "    internal_gap_days: int = 8,  # ç›¸é‚»è®°å½•é—´éš” > 8 å¤©è§†ä¸ºâ€œå¯èƒ½ç¼ºå£â€ï¼ˆå‘¨æœ«/èŠ‚å‡æ—¥ä¸€èˆ¬<4å¤©ï¼‰\n",
    "):\n",
    "    \"\"\"\n",
    "    ä»…ä¸‹è½½ç¼ºå£åŒºé—´ï¼Œæœ€å°åŒ–è¯·æ±‚é‡ï¼›è‡ªåŠ¨å°†æ–°æ•°æ®åˆå¹¶åˆ°æ¯åªè‚¡ç¥¨å·²æœ‰ CSV ä¸­ï¼Œç»“æœæŒ‰æ—¶é—´å‡åºã€‚\n",
    "    - è‹¥ CSV ä¸å­˜åœ¨ï¼šä»…ä¸€æ¬¡è¯·æ±‚ [start, end]\n",
    "    - è‹¥å­˜åœ¨ï¼šåªè¯·æ±‚ (start..min-1)ã€(max+1..end) ä»¥åŠâ€œæ˜æ˜¾å†…éƒ¨åˆ†æ®µç¼ºå£â€\n",
    "    \"\"\"\n",
    "    class RateLimiter:\n",
    "        def __init__(self, rpm: int):\n",
    "            self.interval = 60.0 / max(1, rpm)\n",
    "            self.last = 0.0\n",
    "        def sleep(self):\n",
    "            now = time.time()\n",
    "            wait = self.interval - (now - self.last)\n",
    "            if wait > 0:\n",
    "                time.sleep(wait)\n",
    "            self.last = time.time()\n",
    "\n",
    "    def _sleep_range(rng):\n",
    "        if isinstance(rng, (tuple, list)) and len(rng) == 2:\n",
    "            time.sleep(random.uniform(rng[0], rng[1]))\n",
    "        elif isinstance(rng, (int, float)) and rng > 0:\n",
    "            time.sleep(float(rng))\n",
    "\n",
    "    def _minus_one_day(dt): return dt - timedelta(days=1)\n",
    "    def _plus_one_day(dt):  return dt + timedelta(days=1)\n",
    "\n",
    "    def _merge_write_csv(out_path: str, df_new: pd.DataFrame):\n",
    "        # è¦æ±‚ df_new index=DatetimeIndexï¼ŒåŒ…å«è¡Œæƒ…åˆ—ï¼›ç¨³å¥åˆå¹¶å¹¶å†™å›\n",
    "        if df_new is None or len(df_new) == 0:\n",
    "            return\n",
    "        df_new = df_new.sort_index()\n",
    "        if os.path.exists(out_path):\n",
    "            try:\n",
    "                df_old = pd.read_csv(out_path, parse_dates=[\"date\"]).set_index(\"date\").sort_index()\n",
    "            except Exception:\n",
    "                df_old = pd.DataFrame(index=pd.DatetimeIndex([], name=\"date\"))\n",
    "            # ç»Ÿä¸€åˆ—\n",
    "            all_cols = sorted(set(df_old.columns) | set(df_new.columns))\n",
    "            df_old = df_old.reindex(columns=all_cols)\n",
    "            df_new = df_new.reindex(columns=all_cols)\n",
    "            df_merged = pd.concat([df_old, df_new])\n",
    "        else:\n",
    "            df_merged = df_new\n",
    "        # å»é‡ï¼ˆåŒæ—¥ä¿ç•™æœ€åä¸€æ¡ï¼‰ï¼Œå†æ’åº\n",
    "        df_merged = df_merged[~df_merged.index.duplicated(keep=\"last\")].sort_index()\n",
    "        df_merged.to_csv(out_path, index=True)\n",
    "\n",
    "    def _compute_missing_spans(existing_dates: pd.DatetimeIndex, start_dt: pd.Timestamp, end_dt: pd.Timestamp):\n",
    "        \"\"\"\n",
    "        åŸºäºå·²æœ‰æ•°æ®çš„æœ€æ—©/æœ€æ™šæ—¶é—´ç‚¹ï¼Œå¤–åŠ â€œæ˜æ˜¾å†…éƒ¨åˆ†æ®µç¼ºå£â€ï¼Œç»™å‡ºéœ€è¦è¡¥é½çš„æœ€å°‘è¿ç»­åŒºé—´åˆ—è¡¨ã€‚\n",
    "        æ³¨æ„ï¼šæˆ‘ä»¬ä¸ä¾èµ–äº¤æ˜“æ—¥å†ï¼Œä»¥â€œ> internal_gap_days å¤©â€ä½œä¸ºç¼ºå£é˜ˆå€¼ï¼Œå°½é‡å‡å°‘å†…éƒ¨è¯·æ±‚æ¬¡æ•°ã€‚\n",
    "        \"\"\"\n",
    "        spans = []\n",
    "\n",
    "        if existing_dates is None or len(existing_dates) == 0:\n",
    "            spans.append((start_dt, end_dt))\n",
    "            return spans\n",
    "\n",
    "        dmin, dmax = existing_dates.min(), existing_dates.max()\n",
    "\n",
    "        # 1) å‘å‰å›è¡¥\n",
    "        if start_dt < dmin:\n",
    "            spans.append((start_dt, min(_minus_one_day(dmin), end_dt)))\n",
    "\n",
    "        # 2) å‘åå¢é‡\n",
    "        if end_dt > dmax:\n",
    "            spans.append((max(_plus_one_day(dmax), start_dt), end_dt))\n",
    "\n",
    "        # 3) æ˜æ˜¾å†…éƒ¨åˆ†æ®µç¼ºå£ï¼ˆä»…å½“ç›®æ ‡åŒºé—´è¦†ç›–åˆ°è¿™äº›æ®µï¼‰\n",
    "        # æ’åºåæ‰«æç›¸é‚»æ—¥æœŸå·®\n",
    "        ed = existing_dates[(existing_dates >= start_dt) & (existing_dates <= end_dt)]\n",
    "        ed = ed.sort_values().unique()\n",
    "        for i in range(1, len(ed)):\n",
    "            gap = (ed[i] - ed[i-1]).days\n",
    "            if gap > internal_gap_days:\n",
    "                # è®¤ä¸º (ed[i-1]+1 .. ed[i]-1) å¯èƒ½ç¼ºå¤±ï¼ŒåŠ å…¥ä¸€ä¸ªè¿ç»­åŒºé—´\n",
    "                left  = max(_plus_one_day(ed[i-1]), start_dt)\n",
    "                right = min(_minus_one_day(ed[i]),   end_dt)\n",
    "                if left <= right:\n",
    "                    spans.append((left, right))\n",
    "\n",
    "        # åˆå¹¶å¯èƒ½ç›¸é‚»/é‡å çš„ spansï¼Œå‡å°‘è¯·æ±‚æ¬¡æ•°\n",
    "        if not spans:\n",
    "            return spans\n",
    "        spans = sorted(spans, key=lambda x: x[0])\n",
    "        merged = []\n",
    "        cur_s, cur_e = spans[0]\n",
    "        for s, e in spans[1:]:\n",
    "            if s <= cur_e + timedelta(days=1):\n",
    "                cur_e = max(cur_e, e)\n",
    "            else:\n",
    "                merged.append((cur_s, cur_e))\n",
    "                cur_s, cur_e = s, e\n",
    "        merged.append((cur_s, cur_e))\n",
    "        return merged\n",
    "\n",
    "    # â€”â€” ä¸»æµç¨‹ â€”â€” #\n",
    "    limiter = RateLimiter(rate_per_min)\n",
    "\n",
    "    start = pd.to_datetime(normalize_ymd(start_raw))\n",
    "    end   = pd.to_datetime(normalize_ymd(end_raw))\n",
    "    ensure_dir(DATA_DIR)\n",
    "\n",
    "    codes = load_a_share_codes()\n",
    "    total = len(codes)\n",
    "\n",
    "    if resume_from_symbol:\n",
    "        mask = (codes[\"code\"].astype(str).str.strip() == str(resume_from_symbol).strip())\n",
    "        idxs = np.where(mask.values)[0]\n",
    "        if len(idxs):\n",
    "            start_at = int(idxs[0]) + 2\n",
    "        else:\n",
    "            print(f\"æç¤ºï¼šä»£ç  {resume_from_symbol} ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œæ”¹ç”¨ start_at={start_at}\")\n",
    "\n",
    "    start_at = max(1, min(start_at, total))\n",
    "    iter_df = codes.iloc[start_at-1:]\n",
    "\n",
    "    ok_req = 0     # æˆåŠŸè¯·æ±‚æ¬¡æ•°ï¼ˆåŒºé—´çº§ï¼‰\n",
    "    ok_sym = 0     # è‡³å°‘æœ‰ä¸€æ¬¡æˆåŠŸå†™å…¥çš„è‚¡ç¥¨æ•°\n",
    "    skip_sym = 0   # æ— éœ€ä¸‹è½½çš„è‚¡ç¥¨æ•°\n",
    "    fail_req = 0\n",
    "    hits = 0\n",
    "    recent_fails: list[bool] = []\n",
    "    fail_log = []  # (sym, name, span, err)\n",
    "\n",
    "    print(f\"æœ€å°åŒ–ä¸‹è½½è®¡åˆ’ï¼š{total} åªï¼›ä» {start_at}/{total} å¼€å§‹ï¼›åŒºé—´ {start.date()}~{end.date()}ï¼›å£å¾„ï¼š{adjust or 'none'}\")\n",
    "\n",
    "    global _EM_SESSION\n",
    "\n",
    "    for j, row in enumerate(iter_df.itertuples(index=False), start=start_at):\n",
    "        sym, name = row.code, row.name\n",
    "        out = csv_path(sym)\n",
    "\n",
    "        # â€”â€” æ–­è·¯å™¨ â€”â€” #\n",
    "        if len(recent_fails) >= circuit_fail_window and sum(recent_fails[-circuit_fail_window:]) >= circuit_fail_threshold:\n",
    "            cool = random.uniform(*circuit_sleep_range)\n",
    "            print(f\"â›” è¿ç»­å¤±è´¥è§¦å‘æ–­è·¯å™¨ï¼šä¼‘çœ  {cool:.1f}s å¹¶é‡å»ºä¼šè¯ â€¦\")\n",
    "            time.sleep(cool)\n",
    "            _EM_SESSION = make_session_em()\n",
    "            recent_fails.clear()\n",
    "\n",
    "        # â€”â€” é™é€Ÿ & éšæœºæŠ–åŠ¨ â€”â€” #\n",
    "        limiter.sleep()\n",
    "        _sleep_range(per_item_sleep)\n",
    "\n",
    "        # å‘¨æœŸæ€§è½®æ¢ä¼šè¯\n",
    "        hits += 1\n",
    "        if session_refresh_every > 0 and (hits % session_refresh_every == 0):\n",
    "            print(\"â™»ï¸ è½®æ¢ EM ä¼šè¯ â€¦\")\n",
    "            _EM_SESSION = make_session_em()\n",
    "\n",
    "        # â€”â€” è¯»å–å·²æœ‰æ–‡ä»¶ä»¥ç¡®å®šç¼ºå£ â€”â€” #\n",
    "        existing_idx = None\n",
    "        if os.path.exists(out):\n",
    "            try:\n",
    "                old = pd.read_csv(out, parse_dates=[\"date\"]).set_index(\"date\").sort_index()\n",
    "                existing_idx = old.index\n",
    "            except Exception:\n",
    "                existing_idx = None\n",
    "\n",
    "        spans = _compute_missing_spans(existing_idx, start, end)\n",
    "\n",
    "        if not spans:\n",
    "            print(f\"[{j}/{total}] {sym} {name}ï¼šæ— éœ€ä¸‹è½½ï¼ˆå·²è¦†ç›– {start.date()}~{end.date()}ï¼‰\")\n",
    "            skip_sym += 1\n",
    "            continue\n",
    "\n",
    "        wrote_any = False\n",
    "        for (s_dt, e_dt) in spans:\n",
    "            # å†é˜²å¾¡ï¼šæ— æ•ˆåŒºé—´è·³è¿‡\n",
    "            if s_dt > e_dt:\n",
    "                continue\n",
    "\n",
    "            # â€”â€” åŒºé—´çº§é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰â€”â€” #\n",
    "            last_err = None\n",
    "            success_span = False\n",
    "            for t in range(retry_per_symbol):\n",
    "                try:\n",
    "                    df_new = fetch_hist(sym, s_dt.strftime(\"%Y-%m-%d\"), e_dt.strftime(\"%Y-%m-%d\"), adjust)\n",
    "                    # é˜²å¾¡ï¼šç¡®ä¿ index æ˜¯ DatetimeIndex\n",
    "                    if not isinstance(df_new.index, pd.DatetimeIndex):\n",
    "                        if \"date\" in df_new.columns:\n",
    "                            df_new = df_new.set_index(pd.to_datetime(df_new[\"date\"]))\n",
    "                            df_new.drop(columns=[c for c in [\"date\"] if c in df_new.columns], inplace=True, errors=\"ignore\")\n",
    "                        else:\n",
    "                            # å¦‚æœ fetch_hist è¿”å›æ²¡æœ‰ç´¢å¼•çš„ DataFrameï¼Œå°è¯•æ˜¾å¼ parse\n",
    "                            df_new.index = pd.to_datetime(df_new.index)\n",
    "\n",
    "                    _merge_write_csv(out, df_new)\n",
    "                    ok_req += 1\n",
    "                    success_span = True\n",
    "                    wrote_any = True\n",
    "                    print(f\"[{j}/{total}] {sym} {name}ï¼šè¡¥é½ {s_dt.date()}~{e_dt.date()}ï¼ˆ{len(df_new)} æ¡ï¼‰\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    last_err = e\n",
    "                    msg = str(e)\n",
    "                    # ç½‘ç»œç±»é‡è¯• â†’ æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§ 24s\n",
    "                    if (\"Remote end closed\" in msg) or (\"Connection\" in msg) or (\"Timeout\" in msg):\n",
    "                        backoff = min(6 * (t + 1), 24)\n",
    "                        print(f\"[{j}/{total}] {sym} åŒºé—´ {s_dt.date()}~{e_dt.date()} å‡ºé”™ï¼ˆ{e}ï¼‰ï¼Œ{backoff}s åé‡è¯•({t+1}/{retry_per_symbol})â€¦\")\n",
    "                        time.sleep(backoff)\n",
    "                    else:\n",
    "                        # å…¶ä»–é”™è¯¯å°é€€é¿\n",
    "                        time.sleep(0.6)\n",
    "\n",
    "            if not success_span:\n",
    "                # å†·å´å…œåº•ä¸€æ¬¡\n",
    "                print(f\"[{j}/{total}] {sym} åŒºé—´ {s_dt.date()}~{e_dt.date()} å¸¸è§„é‡è¯•å·²ç”¨å°½ï¼Œå†·å´ 20s åæœ€åå°è¯•ä¸€æ¬¡â€¦\")\n",
    "                time.sleep(20)\n",
    "                try:\n",
    "                    df_new = fetch_hist(sym, s_dt.strftime(\"%Y-%m-%d\"), e_dt.strftime(\"%Y-%m-%d\"), adjust)\n",
    "                    if not isinstance(df_new.index, pd.DatetimeIndex):\n",
    "                        if \"date\" in df_new.columns:\n",
    "                            df_new = df_new.set_index(pd.to_datetime(df_new[\"date\"]))\n",
    "                            df_new.drop(columns=[c for c in [\"date\"] if c in df_new.columns], inplace=True, errors=\"ignore\")\n",
    "                        else:\n",
    "                            df_new.index = pd.to_datetime(df_new.index)\n",
    "                    _merge_write_csv(out, df_new)\n",
    "                    ok_req += 1\n",
    "                    wrote_any = True\n",
    "                    print(f\"[{j}/{total}] {sym} å†·å´é‡è¯•æˆåŠŸï¼š{s_dt.date()}~{e_dt.date()}ï¼ˆ{len(df_new)} æ¡ï¼‰\")\n",
    "                except Exception as e2:\n",
    "                    fail_req += 1\n",
    "                    fail_log.append((sym, name, f\"{s_dt.date()}~{e_dt.date()}\", repr(last_err), repr(e2)))\n",
    "                    print(f\"[{j}/{total}] ä»å¤±è´¥ {sym} {name}ï¼šåŒºé—´ {s_dt.date()}~{e_dt.date()} -> {repr(last_err)}ï¼›å…œåº•ä»å¤±è´¥ï¼š{repr(e2)}\")\n",
    "\n",
    "        recent_fails.append(not wrote_any)\n",
    "        if len(recent_fails) > circuit_fail_window:\n",
    "            recent_fails = recent_fails[-circuit_fail_window:]\n",
    "\n",
    "        if wrote_any:\n",
    "            ok_sym += 1\n",
    "\n",
    "        if cooldown_every > 0 and (hits % cooldown_every == 0):\n",
    "            _sleep_range(cooldown_seconds)\n",
    "            print(f\"ğŸ“¦ å·²å‘èµ· {hits} æ¬¡è¯·æ±‚è§¦å‘å¼ºåˆ¶å†·å´ã€‚\")\n",
    "\n",
    "    print(\"\\nâ€”â€” ä¸‹è½½å®Œæˆï¼ˆæœ€å°åŒ–ç­–ç•¥ï¼‰â€”â€”\")\n",
    "    print(f\"è‚¡ç¥¨è¦†ç›–ï¼šæˆåŠŸå†™å…¥ {ok_sym} åªï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªåŒºé—´è¡¥é½ï¼‰ï¼Œè·³è¿‡ {skip_sym} åªï¼ˆå·²å®Œæ•´ï¼‰ï¼ŒåŒºé—´è¯·æ±‚æˆåŠŸ {ok_req} æ¬¡ï¼Œå¤±è´¥ {fail_req} æ¬¡ã€‚\")\n",
    "    if fail_log:\n",
    "        print(\"\\nä»¥ä¸‹åŒºé—´å¤±è´¥ï¼ˆå»ºè®®ç¨åå•ç‹¬é‡è¯•æˆ–æ¢ç½‘ç»œ/ä»£ç†ï¼‰ï¼š\")\n",
    "        for sym, name, span, e1, e2 in fail_log:\n",
    "            print(f\" - {sym} {name}  {span}  ->  {e1} | {e2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5185833b-2667-4e08-8d23-415a8ac43468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === é…ç½®åŒºï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ =========================================\n",
    "DATA_DIR        = \"ak_hist\"  # å†å²æ•°æ®ä¿å­˜ç›®å½•\n",
    "START_DATE_RAW  = \"20241231\"                   # å…è®¸å†™æˆä¸å­˜åœ¨çš„æ—¥æœŸï¼Œä¼šè‡ªåŠ¨è§„æ•´\n",
    "END_DATE_RAW    = \"20250930\"\n",
    "ADJUST          = \"\"                           # å¤æƒå£å¾„ï¼š\"\" / \"qfq\" / \"hfq\"\n",
    "\n",
    "# å‡çº¿çª—å£ï¼ˆå•ä½ï¼šäº¤æ˜“æ—¥ï¼‰\n",
    "WIN_DAILY  = 5    # â€œæ—¥å‡çº¿â€ = 5æ—¥\n",
    "WIN_WEEKLY = 10   # â€œå‘¨çº¿â€   = 10æ—¥ï¼ˆçº¦ä¸¤å‘¨ï¼‰\n",
    "WIN_MONTH  = 20   # â€œæœˆå‡çº¿â€ = 20æ—¥ï¼ˆçº¦ä¸€æœˆï¼‰\n",
    "\n",
    "# å•åªè‚¡ç¥¨æŠ“å–é‡è¯•\n",
    "#RETRY_PER_SYMBOL = batch_download_all(START_DATE_RAW, END_DATE_RAW, ADJUST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4598439b-0d13-4622-9bba-bd0ab3880532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€å°åŒ–ä¸‹è½½è®¡åˆ’ï¼š5436 åªï¼›ä» 1/5436 å¼€å§‹ï¼›åŒºé—´ 2024-12-31~2025-09-30ï¼›å£å¾„ï¼šnone\n",
      "[1/5436] 000001 åŒºé—´ 2024-12-31~2025-09-30 å¸¸è§„é‡è¯•å·²ç”¨å°½ï¼Œå†·å´ 20s åæœ€åå°è¯•ä¸€æ¬¡â€¦\n",
      "[1/5436] ä»å¤±è´¥ 000001 å¹³å®‰é“¶è¡Œï¼šåŒºé—´ 2024-12-31~2025-09-30 -> RuntimeError('ç¼ºå°‘å¿…è¦åˆ—ï¼š[]')ï¼›å…œåº•ä»å¤±è´¥ï¼šRuntimeError('ç¼ºå°‘å¿…è¦åˆ—ï¼š[]')\n",
      "[2/5436] 000002 åŒºé—´ 2024-12-31~2025-09-30 å¸¸è§„é‡è¯•å·²ç”¨å°½ï¼Œå†·å´ 20s åæœ€åå°è¯•ä¸€æ¬¡â€¦\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbatch_download_all_minimal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTART_DATE_RAW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND_DATE_RAW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mADJUST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_at\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_per_symbol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 227\u001b[0m, in \u001b[0;36mbatch_download_all_minimal\u001b[0;34m(start_raw, end_raw, adjust, start_at, resume_from_symbol, per_item_sleep, rate_per_min, cooldown_every, cooldown_seconds, session_refresh_every, circuit_fail_window, circuit_fail_threshold, circuit_sleep_range, retry_per_symbol, internal_gap_days)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success_span:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# å†·å´å…œåº•ä¸€æ¬¡\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msym\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m åŒºé—´ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms_dt\u001b[38;5;241m.\u001b[39mdate()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m~\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me_dt\u001b[38;5;241m.\u001b[39mdate()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m å¸¸è§„é‡è¯•å·²ç”¨å°½ï¼Œå†·å´ 20s åæœ€åå°è¯•ä¸€æ¬¡â€¦\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 227\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m         df_new \u001b[38;5;241m=\u001b[39m fetch_hist(sym, s_dt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m), e_dt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m), adjust)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_download_all_minimal(START_DATE_RAW, END_DATE_RAW, ADJUST, start_at=1, retry_per_symbol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "878e9722-2358-49fd-97d4-159288397b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_download_all(\n",
    "    start_raw: str, end_raw: str, adjust: str = ADJUST,\n",
    "    start_at: int = 1, resume_from_symbol: str | None = None,\n",
    "    per_item_sleep: tuple[float,float] = (0.6, 1.2),\n",
    "    cooldown_every: int = 50,\n",
    "    cooldown_seconds: tuple[float,float] | float = (1000, 3700),\n",
    "    second_pass: bool = True,\n",
    "    second_pass_cooldown: tuple[float,float] = (200, 260),\n",
    "    # â€”â€” ç¨³æ€å‚æ•° â€”â€” \n",
    "    rate_per_min: int = 15,\n",
    "    session_refresh_every: int = 100,\n",
    "    circuit_fail_window: int = 5,\n",
    "    circuit_fail_threshold: int = 3,\n",
    "    circuit_sleep_range: tuple[float,float] = (1200, 1800),\n",
    "    # â€”â€” æ–°å‚æ•°ï¼šæ¯åªä»£ç çš„å¸¸è§„é‡è¯•æ¬¡æ•°ï¼ˆä¸å«æœ€åå†·å´å…œåº•ï¼‰â€”â€”\n",
    "    retry_per_symbol: int = 3,\n",
    "):\n",
    "    \"\"\"\n",
    "    åˆ†æ‰¹ä¸‹è½½æ‰€æœ‰ A è‚¡å†å²åˆ° CSVï¼Œå¸¦é™é€Ÿ/å†·å´/ä¼šè¯è½®æ¢/æ–­è·¯å™¨/å¤±è´¥äºŒæ‰«ã€‚\n",
    "    \"\"\"\n",
    "    import random, time, os\n",
    "\n",
    "    class RateLimiter:\n",
    "        def __init__(self, rpm: int):\n",
    "            self.interval = 60.0 / max(1, rpm)\n",
    "            self.last = 0.0\n",
    "        def sleep(self):\n",
    "            now = time.time()\n",
    "            wait = self.interval - (now - self.last)\n",
    "            if wait > 0:\n",
    "                time.sleep(wait)\n",
    "            self.last = time.time()\n",
    "\n",
    "    def _sleep_range(rng):\n",
    "        if isinstance(rng, (tuple, list)) and len(rng) == 2:\n",
    "            import random as _r\n",
    "            time.sleep(_r.uniform(rng[0], rng[1]))\n",
    "        elif isinstance(rng, (int, float)) and rng > 0:\n",
    "            time.sleep(float(rng))\n",
    "\n",
    "    limiter = RateLimiter(rate_per_min)\n",
    "\n",
    "    start = normalize_ymd(start_raw)\n",
    "    end   = normalize_ymd(end_raw)\n",
    "    ensure_dir(DATA_DIR)\n",
    "\n",
    "    codes = load_a_share_codes()\n",
    "    total = len(codes)\n",
    "\n",
    "    if resume_from_symbol:\n",
    "        import numpy as np\n",
    "        mask = (codes[\"code\"].astype(str).str.strip() == str(resume_from_symbol).strip())\n",
    "        idxs = np.where(mask.values)[0]\n",
    "        if len(idxs):\n",
    "            start_at = int(idxs[0]) + 2\n",
    "        else:\n",
    "            print(f\"æç¤ºï¼šä»£ç  {resume_from_symbol} ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œæ”¹ç”¨ start_at={start_at}\")\n",
    "\n",
    "    start_at = max(1, min(start_at, total))\n",
    "    iter_df = codes.iloc[start_at-1:]\n",
    "\n",
    "    ok = fail = hits = 0\n",
    "    fail_queue: list[tuple[str,str]] = []\n",
    "    recent_fails: list[bool] = []\n",
    "\n",
    "    print(f\"è®¡åˆ’ä¸‹è½½ï¼š{total} åªï¼›ä»ç¬¬ {start_at}/{total} å¼€å§‹ï¼›åŒºé—´ {start}~{end}ï¼›å£å¾„ï¼š{adjust or 'none'}\")\n",
    "\n",
    "    global _EM_SESSION\n",
    "\n",
    "    # ---------- ç¬¬ä¸€è½® ----------\n",
    "    for j, row in enumerate(iter_df.itertuples(index=False), start=start_at):\n",
    "        sym, name = row.code, row.name\n",
    "        out = csv_path(sym)\n",
    "\n",
    "        if os.path.exists(out):\n",
    "            print(f\"[{j}/{total}] è·³è¿‡ {sym} {name}ï¼ˆå·²å­˜åœ¨ï¼‰\")\n",
    "            continue\n",
    "\n",
    "        # æ–­è·¯å™¨\n",
    "        if len(recent_fails) >= circuit_fail_window and sum(recent_fails[-circuit_fail_window:]) >= circuit_fail_threshold:\n",
    "            import random as _r\n",
    "            cool = _r.uniform(*circuit_sleep_range)\n",
    "            print(f\"â›” è¿ç»­å¤±è´¥è§¦å‘æ–­è·¯å™¨ï¼šä¼‘çœ  {cool:.1f}s å¹¶é‡å»ºä¼šè¯ â€¦\")\n",
    "            time.sleep(cool)\n",
    "            _EM_SESSION = make_session_em()\n",
    "            recent_fails.clear()\n",
    "\n",
    "        limiter.sleep()\n",
    "        _sleep_range(per_item_sleep)\n",
    "\n",
    "        hits += 1\n",
    "        if session_refresh_every > 0 and (hits % session_refresh_every == 0):\n",
    "            print(\"â™»ï¸ è½®æ¢ EM ä¼šè¯ â€¦\")\n",
    "            _EM_SESSION = make_session_em()\n",
    "\n",
    "        last_err = None\n",
    "        success = False\n",
    "\n",
    "        # å¸¸è§„é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰\n",
    "        for t in range(retry_per_symbol):\n",
    "            try:\n",
    "                df = fetch_hist(sym, start, end, adjust)\n",
    "                df.to_csv(out, index=True)\n",
    "                ok += 1\n",
    "                print(f\"[{j}/{total}] ä¿å­˜ {sym} {name}ï¼š{len(df)} æ¡\")\n",
    "                success = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                msg = str(e)\n",
    "                if (\"Remote end closed\" in msg) or (\"Connection\" in msg) or (\"Timeout\" in msg):\n",
    "                    backoff = min(6 * (t + 1), 24)\n",
    "                    print(f\"[{j}/{total}] {sym} å‡ºé”™ï¼ˆ{e}ï¼‰ï¼Œ{backoff}s åé‡è¯•({t+1}/{retry_per_symbol})â€¦\")\n",
    "                    time.sleep(backoff)\n",
    "                else:\n",
    "                    time.sleep(0.6)\n",
    "\n",
    "        if not success:\n",
    "            print(f\"[{j}/{total}] {sym} å¸¸è§„é‡è¯•å·²ç”¨å°½ï¼Œå†·å´ 30s åæœ€åå°è¯•ä¸€æ¬¡â€¦\")\n",
    "            time.sleep(30)\n",
    "            try:\n",
    "                df = fetch_hist(sym, start, end, adjust)\n",
    "                df.to_csv(out, index=True)\n",
    "                ok += 1\n",
    "                print(f\"[{j}/{total}] ä¿å­˜ {sym} {name}ï¼š{len(df)} æ¡ï¼ˆå†·å´é‡è¯•æˆåŠŸï¼‰\")\n",
    "                success = True\n",
    "            except Exception as e2:\n",
    "                fail += 1\n",
    "                fail_queue.append((sym, name))\n",
    "                print(f\"[{j}/{total}] å¤±è´¥ {sym} {name}ï¼š{repr(last_err)}ï¼›å†·å´é‡è¯•ä»å¤±è´¥ï¼š{repr(e2)}\")\n",
    "\n",
    "        recent_fails.append(not success)\n",
    "        if len(recent_fails) > circuit_fail_window:\n",
    "            recent_fails = recent_fails[-circuit_fail_window:]\n",
    "\n",
    "        if success and cooldown_every > 0 and (hits % cooldown_every == 0):\n",
    "            _sleep_range(cooldown_seconds)\n",
    "            print(f\"ğŸ“¦ å·²æŠ“å– {hits} åªï¼Œå®Œæˆå¼ºåˆ¶å†·å´ã€‚\")\n",
    "\n",
    "    print(f\"\\nç¬¬ä¸€è½®å®Œæˆï¼šæˆåŠŸ {ok}ï¼Œå¤±è´¥ {fail}ï¼Œç›®å½•ï¼š{os.path.abspath(DATA_DIR)}\")\n",
    "\n",
    "    # ---------- ç¬¬äºŒè½® ----------\n",
    "    if second_pass and fail_queue:\n",
    "        print(f\"\\nå¼€å§‹äºŒæ¬¡æ‰«æï¼ˆå¤±è´¥ {len(fail_queue)} åªï¼Œæ…¢é€Ÿå†·å´é‡è¯•ï¼‰â€¦\")\n",
    "        still_fail = []\n",
    "        for idx, (sym, name) in enumerate(fail_queue, 1):\n",
    "            print(f\"[äºŒæ¬¡ {idx}/{len(fail_queue)}] å‡†å¤‡é‡è¯• {sym} {name}\")\n",
    "            _sleep_range(second_pass_cooldown)\n",
    "            _EM_SESSION = make_session_em()\n",
    "            try:\n",
    "                df = fetch_hist(sym, start, end, adjust)\n",
    "                df.to_csv(csv_path(sym), index=True)\n",
    "                ok += 1; fail -= 1\n",
    "                print(f\"[äºŒæ¬¡ {idx}/{len(fail_queue)}] æˆåŠŸ {sym}ï¼š{len(df)} æ¡\")\n",
    "            except Exception as e3:\n",
    "                still_fail.append((sym, name, repr(e3)))\n",
    "                print(f\"[äºŒæ¬¡ {idx}/{len(fail_queue)}] ä»å¤±è´¥ {sym}ï¼š{repr(e3)}\")\n",
    "\n",
    "        if still_fail:\n",
    "            print(\"\\nä»¥ä¸‹ä»£ç åœ¨äºŒæ¬¡æ‰«æåä»å¤±è´¥ï¼ˆå»ºè®®ç¨åå•ç‹¬é‡è¯•æˆ–æ¢ç½‘ç»œ/ä»£ç†ï¼‰ï¼š\")\n",
    "            for sym, name, err in still_fail:\n",
    "                print(f\" - {sym} {name} -> {err}\")\n",
    "\n",
    "    print(f\"\\næœ€ç»ˆç»Ÿè®¡ï¼šæˆåŠŸ {ok}ï¼Œå¤±è´¥ {fail}ï¼Œç›®å½•ï¼š{os.path.abspath(DATA_DIR)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5666817-5cec-45ba-96c7-7c5e06b3d409",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®¡åˆ’ä¸‹è½½ï¼š5436 åªï¼›ä»ç¬¬ 1/5436 å¼€å§‹ï¼›åŒºé—´ 20241231~20250930ï¼›å£å¾„ï¼šnone\n",
      "[1/5436] ä¿å­˜ 000001 å¹³å®‰é“¶è¡Œï¼š184 æ¡\n",
      "[2/5436] ä¿å­˜ 000002 ä¸‡  ç§‘ï¼¡ï¼š184 æ¡\n",
      "[3/5436] ä¿å­˜ 000004 *STå›½åï¼š183 æ¡\n",
      "[4/5436] ä¿å­˜ 000006 æ·±æŒ¯ä¸šï¼¡ï¼š184 æ¡\n",
      "[5/5436] ä¿å­˜ 000007 å…¨æ–°å¥½ï¼š184 æ¡\n",
      "[6/5436] ä¿å­˜ 000008 ç¥å·é«˜é“ï¼š184 æ¡\n",
      "[7/5436] ä¿å­˜ 000009 ä¸­å›½å®å®‰ï¼š184 æ¡\n",
      "[8/5436] ä¿å­˜ 000010 ç¾ä¸½ç”Ÿæ€ï¼š184 æ¡\n",
      "[9/5436] ä¿å­˜ 000011 æ·±ç‰©ä¸šAï¼š184 æ¡\n",
      "[10/5436] ä¿å­˜ 000012 å—  ç»ï¼¡ï¼š184 æ¡\n",
      "[11/5436] ä¿å­˜ 000014 æ²™æ²³è‚¡ä»½ï¼š184 æ¡\n",
      "[12/5436] ä¿å­˜ 000016 æ·±åº·ä½³ï¼¡ï¼š175 æ¡\n",
      "[13/5436] ä¿å­˜ 000017 æ·±ä¸­åAï¼š184 æ¡\n",
      "[14/5436] ä¿å­˜ 000019 æ·±ç²®æ§è‚¡ï¼š184 æ¡\n",
      "[15/5436] ä¿å­˜ 000020 æ·±åå‘ï¼¡ï¼š184 æ¡\n",
      "[16/5436] ä¿å­˜ 000021 æ·±ç§‘æŠ€ï¼š184 æ¡\n",
      "[17/5436] ä¿å­˜ 000025 ç‰¹  åŠ›ï¼¡ï¼š184 æ¡\n",
      "[18/5436] ä¿å­˜ 000026 é£äºšè¾¾ï¼š184 æ¡\n",
      "[19/5436] ä¿å­˜ 000027 æ·±åœ³èƒ½æºï¼š184 æ¡\n",
      "[20/5436] ä¿å­˜ 000028 å›½è¯ä¸€è‡´ï¼š184 æ¡\n",
      "[21/5436] ä¿å­˜ 000029 æ·±æ·±æˆ¿ï¼¡ï¼š184 æ¡\n",
      "[22/5436] ä¿å­˜ 000030 å¯Œå¥¥è‚¡ä»½ï¼š184 æ¡\n",
      "[23/5436] ä¿å­˜ 000031 å¤§æ‚¦åŸï¼š184 æ¡\n",
      "[24/5436] ä¿å­˜ 000032 æ·±æ¡‘è¾¾ï¼¡ï¼š184 æ¡\n",
      "[25/5436] ä¿å­˜ 000034 ç¥å·æ•°ç ï¼š184 æ¡\n",
      "[26/5436] ä¿å­˜ 000035 ä¸­å›½å¤©æ¥¹ï¼š184 æ¡\n",
      "[27/5436] ä¿å­˜ 000036 åè”æ§è‚¡ï¼š184 æ¡\n",
      "[28/5436] ä¿å­˜ 000037 æ·±å—ç”µAï¼š184 æ¡\n",
      "[29/5436] ä¿å­˜ 000039 ä¸­é›†é›†å›¢ï¼š184 æ¡\n",
      "[30/5436] ä¿å­˜ 000042 ä¸­æ´²æ§è‚¡ï¼š184 æ¡\n",
      "[31/5436] ä¿å­˜ 000045 æ·±çººç»‡ï¼¡ï¼š184 æ¡\n",
      "[32/5436] ä¿å­˜ 000048 äº¬åŸºæ™ºå†œï¼š184 æ¡\n",
      "[33/5436] ä¿å­˜ 000049 å¾·èµ›ç”µæ± ï¼š184 æ¡\n",
      "[34/5436] ä¿å­˜ 000050 æ·±å¤©é©¬ï¼¡ï¼š184 æ¡\n",
      "[35/5436] ä¿å­˜ 000055 æ–¹å¤§é›†å›¢ï¼š184 æ¡\n",
      "[36/5436] ä¿å­˜ 000056 çš‡åº­å›½é™…ï¼š184 æ¡\n",
      "[37/5436] ä¿å­˜ 000058 æ·± èµ› æ ¼ï¼š184 æ¡\n",
      "[38/5436] ä¿å­˜ 000059 åé”¦è‚¡ä»½ï¼š184 æ¡\n",
      "[39/5436] ä¿å­˜ 000060 ä¸­é‡‘å²­å—ï¼š184 æ¡\n",
      "[40/5436] ä¿å­˜ 000061 å†œ äº§ å“ï¼š184 æ¡\n",
      "[41/5436] ä¿å­˜ 000062 æ·±åœ³åå¼ºï¼š184 æ¡\n",
      "[42/5436] ä¿å­˜ 000063 ä¸­å…´é€šè®¯ï¼š184 æ¡\n",
      "[43/5436] ä¿å­˜ 000065 åŒ—æ–¹å›½é™…ï¼š184 æ¡\n",
      "[44/5436] ä¿å­˜ 000066 ä¸­å›½é•¿åŸï¼š184 æ¡\n",
      "[45/5436] ä¿å­˜ 000068 åæ§èµ›æ ¼ï¼š184 æ¡\n",
      "[46/5436] ä¿å­˜ 000069 åä¾¨åŸï¼¡ï¼š184 æ¡\n",
      "[47/5436] ä¿å­˜ 000070 ç‰¹å‘ä¿¡æ¯ï¼š183 æ¡\n",
      "[48/5436] ä¿å­˜ 000078 æµ·ç‹ç”Ÿç‰©ï¼š184 æ¡\n",
      "[49/5436] ä¿å­˜ 000088 ç› ç”° æ¸¯ï¼š184 æ¡\n",
      "[50/5436] ä¿å­˜ 000089 æ·±åœ³æœºåœºï¼š184 æ¡\n"
     ]
    }
   ],
   "source": [
    "batch_download_all(START_DATE_RAW, END_DATE_RAW, ADJUST, start_at=1, retry_per_symbol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ced07-a9a2-4256-babb-e5d6a5d0d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = find_bull_cross_symbols(lookback=1)\n",
    "print(\"è§¦å‘ä¿¡å·çš„è‚¡ç¥¨ï¼š\", symbols)\n",
    "\n",
    "symbols180 = find_bull_cross_symbols180(lookback=1)\n",
    "print(\"è§¦å‘ä¿¡å·çš„è‚¡ç¥¨newï¼š\", symbols180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83f918-00e9-49c0-bb54-32fc6fa3f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in symbols:\n",
    "    plot_three_ma(i) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
